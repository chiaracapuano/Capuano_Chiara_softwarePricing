{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import files and merge on 'user_id'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_results = pd.read_csv(\"test_results.txt\")\n",
    "test_results.drop(test_results.columns[0], axis=1, inplace=True)\n",
    "user_table = pd.read_csv(\"user_table.txt\")\n",
    "\n",
    "df = test_results.merge(user_table, on ='user_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Profile dataframe to see what columns should be removed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title='Pandas Profiling Report')\n",
    "profile.to_file(output_file='profile.html')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove country, user_id (there are no repeat users, user_id is a column of uniques),\n",
    "lat and long (they correspond to the city, so I just keep the city name)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "df.drop(columns = ['lat', 'long', 'user_id', 'country'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fill Nans strategy:\n",
    "- if 'operative_system' is Nan then replace with most frequent occurrence, based on 'device' column\n",
    "- for all other columns with Nan (timestamp, source) replace with more frequent value\n",
    "\n",
    "The % of missing values in timestamp and source is <0.1%.\n",
    "The % of missing values in operative_system and operative_system is 6.4%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "df['operative_system']=df.groupby('device').operative_system.transform(lambda x: x.fillna(x.mode()[0]))\n",
    "df = df.fillna(df.mode().iloc[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The column timestamp is a string.\n",
    "Some timestamps have minutes and seconds values '60': I replace it with '00'.\n",
    "\n",
    "Then I convert the timestamp column in timestamp format, so I can extract features of interest\n",
    "such as hour, month, weekday, year, week number."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3428c730e2d0>:9: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['weeknum'] = df['timestamp'].dt.week\n"
     ]
    }
   ],
   "source": [
    "df['timestamp'] =df['timestamp'].str.replace('60', '00')\n",
    "df['timestamp'] =pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['weeknum'] = df['timestamp'].dt.week\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check for any seasonality: how many months/years has the test been run for?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test months: 5.0    99072\n",
      "4.0    88840\n",
      "3.0    87440\n",
      "Name: month, dtype: int64\n",
      "Test years: 2015.0    275352\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Test months:',df['month'].value_counts())\n",
    "#check how historical the data is\n",
    "print('Test years:',df['year'].value_counts())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The test is run for three months, so the data should not be affected by any seasonality.\n",
    "Also, the test is run in the same year, so the column year can be dropped.\n",
    "The column timestamp is dropped as well, as all the interesting features have been extracted from it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "df.drop(columns = ['year', 'timestamp'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if the trial users have actually been displayed the price '59', by comparing the\n",
    "lengths of the test columns with the price columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99188\n",
      "98244\n"
     ]
    }
   ],
   "source": [
    "print(len(df[df['test']==1]))\n",
    "print(len(df[df['price']==59.0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the lengths of the columns are not the same as they should be, I remove the inconsistent rows from the dataset:\n",
    "I will only keep the rows where test users were displayed the price 59 and non-test users were displayed the price 39."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "df = df[(df['test']==1) & (df['price']==59.0) | (df['test']==0) & (df['price']==39.0)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I can drop the column price as the column test already contains all the valuable information.\n",
    "Also, I can simply drop the remaining Nans that result from bad timestamp conversions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "df.drop(columns = ['price'], inplace=True)\n",
    "df = df.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 1.\n",
    "\n",
    "I check if there is any statistically significant difference between the test and non-test users in\n",
    "terms of conversion.\n",
    "\n",
    "I obtain a contingency table from the columns test and converted, and I display the\n",
    "row-wise percentage of the users that converted or not in test and non-test case.\n",
    "It is necessary to consider the percentage of conversions rather than sheer numbers, as the number of\n",
    "test users (33%) is less than half the number of non-test users (66%)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.690384625029375e-08\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "data_crosstab = pd.crosstab(df['test'],\n",
    "                            df['converted'],\n",
    "                            margins = False,\n",
    "                            normalize = 'index')\n",
    "chi2, p, dof, ex= chi2_contingency(data_crosstab)\n",
    "\n",
    "print(p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A p-value smaller than the reference p-value 0.05 indicates that the difference is statistically significant.\n",
    "This means that users in the non-test case converted significantly more than the users in the non- test case."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 3.\n",
    "\n",
    "I can run the same chi2 test on data obtained in the first week of the A/B test\n",
    "to see if the result is comparable with the data obtained running the test for 3 months.\n",
    "\n",
    "I therefore retain only the data collected in the first week of the first month of the trial."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min month 3.0\n",
      "min day in month 3 0.0\n"
     ]
    }
   ],
   "source": [
    "print('min month',df['month'].min())\n",
    "days = df[df['month']==3.0]\n",
    "print('min day in month 3',df['weekday'].min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "min_week_num = df['weeknum'].min()\n",
    "first_week = df[(df['month']==3.0) & (df['weeknum']==min_week_num)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% chi2 test in one week span\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.332440638658952e-07\n"
     ]
    }
   ],
   "source": [
    "data_crosstab = pd.crosstab(first_week['test'],\n",
    "                            first_week['converted'],\n",
    "                            margins = False,\n",
    "                            normalize = 'index')\n",
    "chi2, p, dof, ex= chi2_contingency(data_crosstab)\n",
    "\n",
    "print(p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I obtain the outcome performing a chi2 test on the first week of data and in the whole three months of A/B testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 2.\n",
    "\n",
    "I now segment the users to gain further insight into conversions.\n",
    "\n",
    "I begin by dropping the test, month and week number columns, as I decide to not account for any seasonality in the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "df.drop(columns = ['month','weeknum','test'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data I have is basically only categorical. I have no continuous variable to speak of,\n",
    "so I cluster users using K-Modes (variant of K-means used for categorical data).\n",
    "\n",
    "\n",
    "I begin by label-encoding the 'source', 'device', 'operative_system', 'city' columns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df_enc = df\n",
    "to_encode = ['source', 'device', 'operative_system', 'city']\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "df_enc[to_encode] = df_enc[to_encode].apply(le.fit_transform)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before proceeding I profile the new dataframe to check for any striking correlation between columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df_enc, title='Pandas Profiling Report 2')\n",
    "profile.to_file(output_file='profile_encoded.html')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There doesn't seem to be any striking correlation between any of the columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make sure of the number of cluster, I use a dendrogram do study the hierarchical dependencies of the variables."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from matplotlib import style\n",
    "style.use('seaborn-white')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Customer Dendograms\")\n",
    "dend = shc.dendrogram(shc.linkage(df_enc, method='ward'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of clusters chosen is 10, so I fit K-Modes with the parameter n_clusters = 3,\n",
    "and I will cluster users based on its outcome.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "km = KModes(n_clusters=10, n_init = 1, verbose=1)\n",
    "clusters = km.fit_predict(df_enc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I re-encode the categorical variables and then I associate the cluster labels to the dataframe rows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_renc = df_enc.copy()\n",
    "df_renc[to_encode].apply(le.fit_transform)\n",
    "\n",
    "df_ind_res = df_renc.reset_index()\n",
    "\n",
    "clustersDf = pd.DataFrame(clusters)\n",
    "clustersDf.columns = ['cluster_predicted']\n",
    "df_clusters = pd.concat([clustersDf, df_ind_res], axis = 1).reset_index()\n",
    "df_clusters = df_clusters.drop(['index', 'level_0'], axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I plot the clusters and their characteristics by category."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cols_clusters = ['source','device', 'operative_system','converted','city','hour','weekday']\n",
    "\n",
    "\n",
    "for col in df_enc.columns:\n",
    "\n",
    "    if col == 'city':\n",
    "        sns.countplot(x=df_clusters[col],order=df_clusters[col].value_counts()[:25].index,hue=df_clusters['cluster_predicted'])\n",
    "        degrees = 90\n",
    "        plt.xticks(rotation=degrees)\n",
    "        plt.show()\n",
    "    else:\n",
    "        sns.countplot(x=df_clusters[col],order=df_clusters[col].value_counts().index,hue=df_clusters['cluster_predicted'])\n",
    "        degrees = 90\n",
    "        plt.xticks(rotation=degrees)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}